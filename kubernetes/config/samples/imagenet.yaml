apiVersion: elastic.pytorch.org/v1alpha1
kind: ElasticJob
metadata:
  name: imagenet
  namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: "etcd-service:2379"
  minReplicas: 2
  maxReplicas: 2
  replicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        metadata:
          annotations:
            container.apparmor.security.beta.kubernetes.io/elasticjob-worker: unconfined
        apiVersion: v1
        kind: Pod
        spec:
          restartPolicy: "Never"
          maxRetries: "20"  
          tolerations:
          - key: "node.kubernetes.io/unreachable"
            operator: "Exists"
            effect: "NoExecute"
            tolerationSeconds: 10
          - key: "node.kubernetes.io/not-ready"
            operator: "Exists"
            effect: "NoExecute"
            tolerationSeconds: 10
          containers:
            - name: elasticjob-worker
              image: fotstrt/torchelastic #torchelastic/examples:0.2.0
              securityContext:
                 privileged: false
                 capabilities:
                   add:
                     - SYS_ADMIN
              lifecycle:
                postStart:
                  exec:
                    command: ["gcsfuse", "--implicit-dirs", "imagenet-raw-euwest4", "/workspace/data/imagenet-large"]
                preStop:
                  exec:
                    command:
                    - fusermount
                    - -u
                    - /workspace/data/imagenet-large
              imagePullPolicy: Always
              args:
                - "--nproc_per_node=1"
                - "/workspace/examples/imagenet/baseline_imagenet_gcs.py"
                - "--arch=resnet50"
                - "--epochs=90" 
                - "--batch-size=128"
                - "--lr=0.1"
                - "--workers=4"
                - "/workspace/data/imagenet-large"
              resources:
                requests:
                    nvidia.com/gpu: 1
                    smarter-devices/fuse: 1
                limits:
                    nvidia.com/gpu: 1
                    smarter-devices/fuse: 1
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm  
          initContainers:
            - name: torch-test
              image: pytorch/pytorch:1.8.0-cuda11.1-cudnn8-runtime
              command: ['python', '-c', 'import torch; a=torch.cuda.is_available(); assert a']
              resources:
                requests:
                  nvidia.com/gpu: 1
                limits:
                  nvidia.com/gpu: 1
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
